{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.parsing.preprocessing import strip_punctuation, strip_numeric\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Data comes from [this academic source](http://fakenews.research.sfu.ca/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/snopes_phase2_clean_2018_7_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = list(df['original_article_text_phase2'])\n",
    "labels = list(df['fact_rating_phase1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 15804 total texts in our dataset.\n"
     ]
    }
   ],
   "source": [
    "print('We have '+str(len(raw_texts))+' total texts in our dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    return strip_punctuation(doc).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [clean(doc) for doc in raw_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates TaggedDocument objects for each of the texts in the dataset, where each text is tagged by the fact rating (label),e.g. \"true\" or \"false.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [label]) for doc,label in zip(texts,labels)]\n",
    "random.shuffle(documents)\n",
    "n = len(documents)\n",
    "split = n*7//10\n",
    "train_corpus = documents[:split]\n",
    "test_corpus = documents[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The model is trained on the documents, with vector size of 100 (for each word), with a window of 10 (each word is predicted by the 10 words surrounding it). min_count = 2 means that every word will be used if it appears more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=100, window=10, min_count=2, epochs=100)\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/my_doc2vec_model\")\n",
    "model = Doc2Vec.load(\"models/my_doc2vec_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.77026314  0.3245291   0.46236303  0.6530718   0.252805    1.1175214\n",
      " -0.82519925 -0.42763317  0.2310017  -0.6000125   0.05518538 -0.5051248\n",
      " -0.11029471 -0.2755155   0.02550256  0.55742896  0.47391373  0.47023472\n",
      "  0.15100312  0.532407    0.24265862  0.28292423 -0.6275333  -0.1948347\n",
      "  0.21949245  0.11276026 -0.1754544  -1.1636639  -0.05264677 -0.11232027\n",
      " -1.316966   -0.35494038  0.06453314  0.3031587  -0.6171715   0.8219609\n",
      " -0.39230192 -0.63734394  0.07676978 -0.78597903 -0.27953026 -0.7963541\n",
      " -0.1812085   0.64095634 -0.58655846 -0.63447726  0.4350234  -0.23559555\n",
      "  0.8868153   0.696059    0.01137678 -0.73044     0.720216   -0.45125455\n",
      "  0.46951807 -0.18292809 -0.8259651   0.35229456  0.04092822 -0.26731676\n",
      " -0.10490963 -0.37612042 -0.2486074   0.03884813  0.6809351  -0.17610945\n",
      "  0.36686182  0.65326095 -0.16514468  0.57372344 -0.0118232  -0.61706966\n",
      "  0.16035254  0.15476711 -0.0227529  -0.8314403   0.6950987  -0.02919235\n",
      " -0.43318322  0.57178324 -0.50574094  0.35658738 -0.08718287 -0.01545667\n",
      "  0.3326992   0.4662214   0.88851124  0.15504627  0.31497023  0.16130514\n",
      "  0.54289705 -0.6373347  -0.40157858  0.39435127  0.14952953  0.05814374\n",
      "  0.23303989 -0.19335732  0.63695735 -0.26978508]\n"
     ]
    }
   ],
   "source": [
    "new_doc = 'hillary clinton won the presidential election'.split()\n",
    "vector = model.infer_vector(new_doc)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "We do the following to make sure the model is behaving in a useful way. For each document in the train corpus, we infer a new vector from the model, calculate the most similar document vectors in the model, and determine if the inferred vectors are closest to themselves in the model. ***rank*** will store the index of the correct document in the similarity list. We should see most of the documents ranked as the number one most similar document to themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(train_corpus[doc_id].tags[0])\n",
    "    ranks.append(rank)\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 8662, 1: 1245, 2: 554, 3: 259, 4: 154, 5: 77, 6: 42, 7: 25, 8: 22, 9: 18, 10: 4})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
