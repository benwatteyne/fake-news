{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import tldextract\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "import editdistance\n",
    "\n",
    "from tqdm import tqdm\n",
    "import bs4\n",
    "import requests\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.features = pd.DataFrame()\n",
    "        n = 2\n",
    "        self.bigram = MLE(n)\n",
    "\n",
    "    def __URLsplit(self,s):\n",
    "        return [char for char in s]\n",
    "\n",
    "    def __buildBigram(self,urls):\n",
    "        train_data, padded_sents = padded_everygram_pipeline(2, urls)\n",
    "        self.bigram.fit(train_data,padded_sents)\n",
    "\n",
    "    def __cleanURL(self,url):\n",
    "        xtract = tldextract.extract(url)\n",
    "        return '.'.join(xtract)\n",
    "\n",
    "    def __editDistance(self,url):\n",
    "        popular_sites = ['https://news.yahoo.com/','https://news.google.com/?hl=en-US&gl=US&ceid=US:en',\n",
    "                        'https://www.huffpost.com/','https://www.cnn.com/','https://www.nytimes.com/',\n",
    "                        'https://www.foxnews.com/','https://www.nbcnews.com/',\n",
    "                        'https://www.dailymail.co.uk/ushome/index.html','https://www.washingtonpost.com/',\n",
    "                        'https://www.theguardian.com/us','https://www.wsj.com/','https://abcnews.go.com/',\n",
    "                        'https://www.bbc.co.uk/news','https://www.usatoday.com/',\n",
    "                        'https://www.latimes.com/']\n",
    "        popular_sites = [self.__cleanURL(str(x)) for x in popular_sites]\n",
    "        dist = float('inf')\n",
    "        for site in popular_sites:\n",
    "            new_dist = editdistance.eval(url,site)\n",
    "            if new_dist < dist:\n",
    "                dist = new_dist\n",
    "        return dist\n",
    "\n",
    "    def __htmlInfo(self,urls):\n",
    "        n = len(urls)\n",
    "        status_codes = [-1]*n\n",
    "        is_active = [0]*n\n",
    "        has_wp_content = [-1]*n\n",
    "        num_iframes = [-1]*n\n",
    "        it = -1\n",
    "        for url in tqdm(urls):\n",
    "            it += 1\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                status_codes[it] = response.status_code\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    page = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "                    is_active[it] = 1\n",
    "                    iframes = page.find_all(name='iframe')\n",
    "                    num_iframes[it] = len(iframes)\n",
    "                    has_wp_content[it] = 1 if response.text.find('wp-content') > -1 else 0\n",
    "            except:\n",
    "                continue\n",
    "        self.features['status'] = status_codes\n",
    "        self.features['active'] = is_active\n",
    "        self.features['wp_content'] = has_wp_content\n",
    "        self.features['num_iframes'] = num_iframes\n",
    "\n",
    "    def __cleanHeadline(self,h):\n",
    "        return remove_stopwords(strip_punctuation(strip_numeric(str(h).lower()))).split(' ')\n",
    "\n",
    "    def __get_val(self,v,row,i):\n",
    "        if v[row] == []:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(v[row][i])\n",
    "\n",
    "    def __headerEmbeddings(self,headers):\n",
    "        header_model = Word2Vec.load(\"models/headline_word_embeddings.model\")\n",
    "        head_vecs = []\n",
    "        for h in headers:\n",
    "            h = self.__cleanHeadline(h)\n",
    "            h = [x for x in h if x in header_model.wv.vocab]\n",
    "            if len(h) >= 1:\n",
    "                head_vecs.append(np.mean(header_model[h],axis=0))\n",
    "            else:\n",
    "                head_vecs.append([])\n",
    "        for i in range(len(head_vecs[0])):\n",
    "            self.features.insert(i,'h_vec_'+str(i),[self.__get_val(head_vecs,row,i) for row in range(len(head_vecs))],True)\n",
    "\n",
    "    def __articleEmbeddings(self,articles):\n",
    "        doc_model = Doc2Vec(vector_size=100, window=10, min_count=2, epochs=100)\n",
    "        doc_model = Doc2Vec.load(\"models/my_doc2vec_model\")\n",
    "        a_vec_labels = []\n",
    "        for i in range(0,100):\n",
    "            a_vec_labels.append('a_vec_'+str(i))\n",
    "        vecs = []\n",
    "        for text in articles:\n",
    "            t = text.split()\n",
    "            e = list(doc_model.infer_vector(t))\n",
    "            vecs.append(e)\n",
    "        a_embeds = pd.DataFrame(vecs,columns=a_vec_labels)\n",
    "        self.features = a_embeds.join(self.features)\n",
    "\n",
    "    def create(self,data,url_col,article_col,header_col=None):\n",
    "        '''\n",
    "        Creates feature dataset from news article URL\n",
    "        Features:\n",
    "          BUILT:\n",
    "            TRANSFERRED:\n",
    "              - bigram entropy\n",
    "              - bigram perplexity\n",
    "              - clean bigram entropy\n",
    "              - clean bigram perplexity\n",
    "              - edit distance to top 15 site\n",
    "              - status\n",
    "              - active\n",
    "              - has wordpress content\n",
    "              - number of iframes\n",
    "            NEW:\n",
    "              - header embeddings\n",
    "          TO BE BUILT:\n",
    "            - article embeddings\n",
    "            - url embeddings\n",
    "        '''\n",
    "        # HEADLINE VECTORS\n",
    "        if header_col:\n",
    "            sys.stdout.write('Building embeddings for headlines...\\n')\n",
    "            self.__headerEmbeddings(data[header_col])\n",
    "\n",
    "        # BIGRAM ENTROPY & PERPLEXITY\n",
    "        sys.stdout.write('Building bigram model features for URL strings...\\n')\n",
    "        urls = data[url_col].apply(lambda a: str(a))\n",
    "        split_urls = urls.apply(lambda a: self.__URLsplit(a))\n",
    "        self.__buildBigram(split_urls)\n",
    "        self.features['bigram_entropy'] = [self.bigram.entropy(x) for x in urls]\n",
    "        self.features['bigram_perplexity'] = [self.bigram.perplexity(x) for x in urls]\n",
    "\n",
    "        # CLEAN BIGRAM ENTROPY & PERPLEXITY\n",
    "        clean_urls = urls.apply(lambda a: self.__cleanURL(str(a)))\n",
    "        split_clean_urls = clean_urls.apply(lambda a: self.__URLsplit(a))\n",
    "        self.__buildBigram(split_clean_urls)\n",
    "        self.features['clean_bigram_entropy'] = [self.bigram.entropy(x) for x in split_clean_urls]\n",
    "        self.features['clean_bigram_perplexity'] = [self.bigram.perplexity(x) for x in split_clean_urls]\n",
    "\n",
    "        # EDIT DISTANCE\n",
    "        sys.stdout.write('Calculating edit distance for each URL string...\\n')\n",
    "        self.features['edit_distance'] = [self.__editDistance(x) for x in clean_urls]\n",
    "\n",
    "        # HTML INFO (STATUS, ACTIVE, WP CONTENT, # IFRAMES)\n",
    "        #sys.stdout.write('Accessing request info for features...\\n')\n",
    "        #self.__htmlInfo(urls)\n",
    "        \n",
    "        # ARTICLE EMBEDDINGS VIA DOC2VEC\n",
    "        sys.stdout.write('Inferring article embeddings via doc2vec...\\n')\n",
    "        self.__articleEmbeddings(data[article_col])\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/snopes_phase2_clean_2018_7_3.csv\")\n",
    "subset = df.sample(100)\n",
    "embeddings = FeatureEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embeddings for headlines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:76: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:64: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building bigram model features for URL strings...\n",
      "Calculating edit distance for each URL string...\n",
      "Inferring article embeddings via doc2vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:00<00:10,  9.65it/s]\u001b[A\n",
      "  3%|▎         | 3/100 [00:00<00:10,  9.17it/s]\u001b[A\n",
      "  4%|▍         | 4/100 [00:00<00:14,  6.82it/s]\u001b[A\n",
      "  6%|▌         | 6/100 [00:00<00:12,  7.76it/s]\u001b[A\n",
      "  8%|▊         | 8/100 [00:01<00:15,  5.94it/s]\u001b[A\n",
      " 10%|█         | 10/100 [00:01<00:12,  7.14it/s]\u001b[A\n",
      " 12%|█▏        | 12/100 [00:01<00:11,  7.85it/s]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:01<00:12,  6.94it/s]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:01<00:11,  7.25it/s]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:02<00:09,  8.66it/s]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:02<00:09,  9.07it/s]\u001b[A\n",
      " 20%|██        | 20/100 [00:02<00:08,  9.68it/s]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:02<00:07, 11.13it/s]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:03<00:12,  6.03it/s]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:03<00:11,  6.79it/s]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:03<00:09,  7.78it/s]\u001b[A\n",
      " 29%|██▉       | 29/100 [00:03<00:11,  6.10it/s]\u001b[A\n",
      " 32%|███▏      | 32/100 [00:04<00:09,  7.51it/s]\u001b[A\n",
      " 35%|███▌      | 35/100 [00:04<00:08,  7.87it/s]\u001b[A\n",
      " 37%|███▋      | 37/100 [00:04<00:07,  8.75it/s]\u001b[A\n",
      " 39%|███▉      | 39/100 [00:04<00:06,  9.12it/s]\u001b[A\n",
      " 41%|████      | 41/100 [00:05<00:05, 10.07it/s]\u001b[A\n",
      " 43%|████▎     | 43/100 [00:05<00:05, 11.14it/s]\u001b[A\n",
      " 45%|████▌     | 45/100 [00:05<00:05, 10.66it/s]\u001b[A\n",
      " 47%|████▋     | 47/100 [00:05<00:05,  9.49it/s]\u001b[A\n",
      " 49%|████▉     | 49/100 [00:06<00:08,  5.98it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:06<00:08,  6.12it/s]\u001b[A\n",
      " 51%|█████     | 51/100 [00:06<00:07,  6.48it/s]\u001b[A\n",
      " 52%|█████▏    | 52/100 [00:06<00:06,  7.23it/s]\u001b[A\n",
      " 53%|█████▎    | 53/100 [00:07<00:09,  4.95it/s]\u001b[A\n",
      " 54%|█████▍    | 54/100 [00:07<00:09,  4.94it/s]\u001b[A\n",
      " 55%|█████▌    | 55/100 [00:07<00:07,  5.79it/s]\u001b[A\n",
      " 56%|█████▌    | 56/100 [00:07<00:09,  4.87it/s]\u001b[A\n",
      " 58%|█████▊    | 58/100 [00:07<00:07,  5.75it/s]\u001b[A\n",
      " 59%|█████▉    | 59/100 [00:08<00:15,  2.60it/s]\u001b[A\n",
      " 61%|██████    | 61/100 [00:08<00:11,  3.41it/s]\u001b[A\n",
      " 62%|██████▏   | 62/100 [00:09<00:10,  3.74it/s]\u001b[A\n",
      " 63%|██████▎   | 63/100 [00:09<00:09,  3.73it/s]\u001b[A\n",
      " 64%|██████▍   | 64/100 [00:09<00:09,  3.76it/s]\u001b[A\n",
      " 65%|██████▌   | 65/100 [00:09<00:08,  4.12it/s]\u001b[A\n",
      " 66%|██████▌   | 66/100 [00:10<00:09,  3.74it/s]\u001b[A\n",
      " 67%|██████▋   | 67/100 [00:10<00:07,  4.45it/s]\u001b[A\n",
      " 68%|██████▊   | 68/100 [00:10<00:10,  3.04it/s]\u001b[A\n",
      " 69%|██████▉   | 69/100 [00:10<00:08,  3.70it/s]\u001b[A\n",
      " 70%|███████   | 70/100 [00:11<00:07,  4.22it/s]\u001b[A\n",
      " 71%|███████   | 71/100 [00:11<00:07,  4.11it/s]\u001b[A\n",
      " 73%|███████▎  | 73/100 [00:11<00:05,  5.22it/s]\u001b[A\n",
      " 75%|███████▌  | 75/100 [00:11<00:03,  6.29it/s]\u001b[A\n",
      " 76%|███████▌  | 76/100 [00:11<00:04,  5.75it/s]\u001b[A\n",
      " 77%|███████▋  | 77/100 [00:12<00:05,  4.59it/s]\u001b[A\n",
      " 78%|███████▊  | 78/100 [00:12<00:05,  4.31it/s]\u001b[A\n",
      " 80%|████████  | 80/100 [00:12<00:03,  5.21it/s]\u001b[A\n",
      " 82%|████████▏ | 82/100 [00:12<00:02,  6.69it/s]\u001b[A\n",
      " 84%|████████▍ | 84/100 [00:12<00:02,  7.34it/s]\u001b[A\n",
      " 86%|████████▌ | 86/100 [00:13<00:01,  7.64it/s]\u001b[A\n",
      " 87%|████████▋ | 87/100 [00:13<00:02,  6.07it/s]\u001b[A\n",
      " 89%|████████▉ | 89/100 [00:13<00:01,  7.36it/s]\u001b[A\n",
      " 91%|█████████ | 91/100 [00:13<00:01,  8.18it/s]\u001b[A\n",
      " 93%|█████████▎| 93/100 [00:14<00:01,  5.35it/s]\u001b[A\n",
      " 94%|█████████▍| 94/100 [00:14<00:01,  4.97it/s]\u001b[A\n",
      " 96%|█████████▌| 96/100 [00:14<00:00,  6.05it/s]\u001b[A\n",
      " 98%|█████████▊| 98/100 [00:14<00:00,  7.65it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:15<00:00,  6.54it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "embeddings.create(subset,article_col='original_article_text_phase2',url_col='article_origin_url_phase1',header_col='article_title_phase2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"data/snopes_phase2_clean_2018_7_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = FeatureEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings.create(df,article_col='original_article_text_phase2',url_col='article_origin_url_phase1',header_col='article_title_phase2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings.features.to_csv(\"embeddings/snopes_phase2_clean_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
