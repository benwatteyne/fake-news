{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import tldextract\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "import editdistance\n",
    "\n",
    "from tqdm import tqdm\n",
    "import bs4\n",
    "import requests\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.features = pd.DataFrame()\n",
    "        n = 2\n",
    "        self.bigram = MLE(n)\n",
    "\n",
    "    def __URLsplit(self,s):\n",
    "        return [char for char in s]\n",
    "\n",
    "    def __buildBigram(self,urls):\n",
    "        train_data, padded_sents = padded_everygram_pipeline(2, urls)\n",
    "        self.bigram.fit(train_data,padded_sents)\n",
    "\n",
    "    def __cleanURL(self,url):\n",
    "        xtract = tldextract.extract(url)\n",
    "        return '.'.join(xtract)\n",
    "\n",
    "    def __editDistance(self,url):\n",
    "        popular_sites = ['https://news.yahoo.com/','https://news.google.com/?hl=en-US&gl=US&ceid=US:en',\n",
    "                        'https://www.huffpost.com/','https://www.cnn.com/','https://www.nytimes.com/',\n",
    "                        'https://www.foxnews.com/','https://www.nbcnews.com/',\n",
    "                        'https://www.dailymail.co.uk/ushome/index.html','https://www.washingtonpost.com/',\n",
    "                        'https://www.theguardian.com/us','https://www.wsj.com/','https://abcnews.go.com/',\n",
    "                        'https://www.bbc.co.uk/news','https://www.usatoday.com/',\n",
    "                        'https://www.latimes.com/']\n",
    "        popular_sites = [self.__cleanURL(str(x)) for x in popular_sites]\n",
    "        dist = float('inf')\n",
    "        for site in popular_sites:\n",
    "            new_dist = editdistance.eval(url,site)\n",
    "            if new_dist < dist:\n",
    "                dist = new_dist\n",
    "        return dist\n",
    "\n",
    "    def __htmlInfo(self,urls):\n",
    "        n = len(urls)\n",
    "        status_codes = [-1]*n\n",
    "        is_active = [0]*n\n",
    "        has_wp_content = [-1]*n\n",
    "        num_iframes = [-1]*n\n",
    "        it = -1\n",
    "        for url in tqdm(urls):\n",
    "            it += 1\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                status_codes[it] = response.status_code\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    page = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "                    is_active[it] = 1\n",
    "                    iframes = page.find_all(name='iframe')\n",
    "                    num_iframes[it] = len(iframes)\n",
    "                    has_wp_content[it] = 1 if response.text.find('wp-content') > -1 else 0\n",
    "            except:\n",
    "                continue\n",
    "        self.features['status'] = status_codes\n",
    "        self.features['active'] = is_active\n",
    "        self.features['wp_content'] = has_wp_content\n",
    "        self.features['num_iframes'] = num_iframes\n",
    "\n",
    "    def __cleanHeadline(self,h):\n",
    "        return remove_stopwords(strip_punctuation(strip_numeric(str(h).lower()))).split(' ')\n",
    "\n",
    "    def __get_val(self,v,row,i):\n",
    "        if v[row] == []:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(v[row][i])\n",
    "\n",
    "    def __headerEmbeddings(self,headers):\n",
    "        header_model = Word2Vec.load(\"models/headline_word_embeddings.model\")\n",
    "        head_vecs = []\n",
    "        for h in headers:\n",
    "            h = self.__cleanHeadline(h)\n",
    "            h = [x for x in h if x in header_model.wv.vocab]\n",
    "            if len(h) >= 1:\n",
    "                head_vecs.append(np.mean(header_model[h],axis=0))\n",
    "            else:\n",
    "                head_vecs.append([])\n",
    "        for i in range(len(head_vecs[0])):\n",
    "            self.features.insert(i,'h_vec_'+str(i),[self.__get_val(head_vecs,row,i) for row in range(len(head_vecs))],True)\n",
    "\n",
    "    def __articleEmbeddings(self,articles):\n",
    "        doc_model = Doc2Vec(vector_size=100, window=10, min_count=2, epochs=100)\n",
    "        doc_model = Doc2Vec.load(\"models/my_doc2vec_model\")\n",
    "        a_vec_labels = []\n",
    "        for i in range(0,100):\n",
    "            a_vec_labels.append('a_vec_'+str(i))\n",
    "        vecs = []\n",
    "        for text in articles:\n",
    "            t = text.split()\n",
    "            e = list(doc_model.infer_vector(t))\n",
    "            vecs.append(e)\n",
    "        a_embeds = pd.DataFrame(vecs,columns=a_vec_labels)\n",
    "        self.features = a_embeds.join(self.features)\n",
    "\n",
    "    def create(self,data,url_col,article_col,header_col=None):\n",
    "        '''\n",
    "        Creates feature dataset from news article URL\n",
    "        Features:\n",
    "          BUILT:\n",
    "            TRANSFERRED:\n",
    "              - bigram entropy\n",
    "              - bigram perplexity\n",
    "              - clean bigram entropy\n",
    "              - clean bigram perplexity\n",
    "              - edit distance to top 15 site\n",
    "              - status\n",
    "              - active\n",
    "              - has wordpress content\n",
    "              - number of iframes\n",
    "            NEW:\n",
    "              - header embeddings\n",
    "          TO BE BUILT:\n",
    "            - article embeddings\n",
    "            - url embeddings\n",
    "        '''\n",
    "        # HEADLINE VECTORS\n",
    "        if header_col:\n",
    "            sys.stdout.write('Building embeddings for headlines...\\n')\n",
    "            self.__headerEmbeddings(data[header_col])\n",
    "\n",
    "        # BIGRAM ENTROPY & PERPLEXITY\n",
    "        sys.stdout.write('Building bigram model features for URL strings...\\n')\n",
    "        urls = data[url_col].apply(lambda a: str(a))\n",
    "        split_urls = urls.apply(lambda a: self.__URLsplit(a))\n",
    "        self.__buildBigram(split_urls)\n",
    "        self.features['bigram_entropy'] = [self.bigram.entropy(x) for x in urls]\n",
    "        self.features['bigram_perplexity'] = [self.bigram.perplexity(x) for x in urls]\n",
    "\n",
    "        # CLEAN BIGRAM ENTROPY & PERPLEXITY\n",
    "        clean_urls = urls.apply(lambda a: self.__cleanURL(str(a)))\n",
    "        split_clean_urls = clean_urls.apply(lambda a: self.__URLsplit(a))\n",
    "        self.__buildBigram(split_clean_urls)\n",
    "        self.features['clean_bigram_entropy'] = [self.bigram.entropy(x) for x in split_clean_urls]\n",
    "        self.features['clean_bigram_perplexity'] = [self.bigram.perplexity(x) for x in split_clean_urls]\n",
    "\n",
    "        # EDIT DISTANCE\n",
    "        sys.stdout.write('Calculating edit distance for each URL string...\\n')\n",
    "        self.features['edit_distance'] = [self.__editDistance(x) for x in clean_urls]\n",
    "\n",
    "        # HTML INFO (STATUS, ACTIVE, WP CONTENT, # IFRAMES)\n",
    "        sys.stdout.write('Accessing request info for features...\\n')\n",
    "        self.__htmlInfo(urls)\n",
    "        \n",
    "        # ARTICLE EMBEDDINGS VIA DOC2VEC\n",
    "        sys.stdout.write('Inferring article embeddings via doc2vec...\\n')\n",
    "        self.__articleEmbeddings(data[article_col])\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/snopes_phase2_clean_2018_7_3.csv\")\n",
    "subset = df.sample(100)\n",
    "embeddings = FeatureEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embeddings for headlines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:76: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:64: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building bigram model features for URL strings...\n",
      "Calculating edit distance for each URL string...\n",
      "Inferring article embeddings via doc2vec...\n"
     ]
    }
   ],
   "source": [
    "embeddings.create(subset,article_col='original_article_text_phase2',url_col='article_origin_url_phase1',header_col='article_title_phase2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"embeddings/full_test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['a_vec_0', 'a_vec_1', 'a_vec_2', 'a_vec_3', 'a_vec_4', 'a_vec_5',\n",
       "       'a_vec_6', 'a_vec_7', 'a_vec_8', 'a_vec_9',\n",
       "       ...\n",
       "       'h_vec_96', 'h_vec_97', 'h_vec_98', 'h_vec_99', 'bigram_entropy',\n",
       "       'bigram_perplexity', 'clean_bigram_entropy', 'clean_bigram_perplexity',\n",
       "       'edit_distance', 'target'],\n",
       "      dtype='object', length=206)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = FeatureEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings.create(df,article_col='original_article_text_phase2',url_col='article_origin_url_phase1',header_col='article_title_phase2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings.features.to_csv(\"embeddings/snopes_phase2_clean_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
